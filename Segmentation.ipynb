{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6567fbc8",
   "metadata": {},
   "source": [
    "# Final Segmentation & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c048289",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2a0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchio as tio\n",
    "\n",
    "from obb import OBB\n",
    "from itkwidgets import view\n",
    "from ipywidgets import widgets\n",
    "\n",
    "from experiment import SegmentationModel3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.ndimage import label, generate_binary_structure\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy import ndimage\n",
    "import nibabel as nib\n",
    "from statistics import mean\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76894c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load latest checkpoint from our model\n",
    "path = \"../checkpoints/confused-elevator-180/epoch=197-avg_train_jaccard=0.00.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c1d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set default hyperparameters\n",
    "defaults = {\n",
    "    'learning_rate': 0.0001,\n",
    "    'loss': 'dice',\n",
    "    'alpha': 0.9,\n",
    "    'blocks': 4,\n",
    "    'batch_size': 2,\n",
    "    'initial_features': 64,\n",
    "\n",
    "    'p_dropout': 0.0,\n",
    "\n",
    "    'p_affine_or_elastic': 0.0,\n",
    "    'p_elastic': 0.2,\n",
    "    'p_affine': 0.8,\n",
    "\n",
    "    'patch_size': 48,\n",
    "    'samples_per_volume': 10,\n",
    "    'queue_length': 80,\n",
    "    'patch_overlap': 4,\n",
    "    'random_sample_ratio': 4,\n",
    "\n",
    "    'log_image_every_n': 3,\n",
    "\n",
    "    'data_path': '/data/training',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4146dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from checkpoint\n",
    "checkpoint = torch.load(path)\n",
    "model = SegmentationModel3d(defaults)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b7704",
   "metadata": {},
   "source": [
    "## 1. Load data and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d65597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files\n",
    "test_files = list(Path('/data/test').glob('*.*'))\n",
    "files_orig = sorted(\n",
    "    list(filter(lambda file: 'orig.nii.gz' in str(file), test_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051d603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset and loader\n",
    "preprocessing = tio.RescaleIntensity(out_min_max=(0, 1))\n",
    "subjects = [\n",
    "    tio.Subject(\n",
    "        t1=tio.ScalarImage(orig)\n",
    "    )\n",
    "    for orig in files_orig]\n",
    "\n",
    "test_set =  tio.SubjectsDataset(subjects, transform=preprocessing)   \n",
    "test_loader = DataLoader(test_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca3875e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform dataset to TorchIO subject with affine matrix and path\n",
    "def transform_to_subject(batch):\n",
    "    batch['t1'] = tio.Image(tensor=batch['t1']['data'][0].to('cuda:0'), affine=batch['t1']['affine'][0], path=batch['t1']['path'])\n",
    "    return tio.Subject(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57590fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define batch prediction\n",
    "def predict_on_batch(batch):\n",
    "    subj = transform_to_subject(batch)\n",
    "    with torch.no_grad():\n",
    "        pred = model(subj)\n",
    "    return (subj, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a090018",
   "metadata": {},
   "source": [
    "## 2. Define Post-Processing\n",
    "As the output of our model is ocassionally noisy, we employ multiple forms of post-processing on the outputs: thresholding, opening and sparseness analysis. Additionally we need to separate multiple aneurysms that might be occuring in one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45cf9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(result):\n",
    "    '''\n",
    "        Postprocessing the model output\n",
    "        :param result: The raw model segmentation output for an entire volume\n",
    "        :return: Individual bounding box and NIFTI mask per found aneurysm\n",
    "    '''\n",
    "    #load image, affine and id from subject\n",
    "    subj = result[0]\n",
    "    affine = subj['t1'].affine\n",
    "    cid = subj['t1'].path[0].stem[0:4]\n",
    "    #exception for one case\n",
    "    if cid == 'A144':\n",
    "        cid = subj['t1'].path[0].stem[0:6]\n",
    "    \n",
    "    #Set dummy for processing time\n",
    "    ptime = -1\n",
    "    \n",
    "    #threshold prediction volume to be above a certain probability\n",
    "    pred = result[1].numpy() > 0.99\n",
    "    \n",
    "    #perform morphological opening\n",
    "    pred = ndimage.binary_opening(pred[0],iterations=3)\n",
    "    #convert to binary array\n",
    "    pred = pred.astype('int')\n",
    "\n",
    "    #find connected components in prediction mask\n",
    "    labeled_array, num_features = label(pred)\n",
    "    #initialize empty list to store the detected aneurysms\n",
    "    detected = []\n",
    "        \n",
    "    accepted_aneurysms = 0\n",
    "    #loop through separate features in prediction\n",
    "    for f in range(num_features):\n",
    "        #get data points from aneurysm\n",
    "        m = np.where(labeled_array == f+1,labeled_array,0)\n",
    "        if np.count_nonzero(m) < 500: \n",
    "            #if aneurysm is very sparse it's probably noise, so we discard it\n",
    "            continue\n",
    "\n",
    "        #save nifti image of mask\n",
    "        acc = str(accepted_aneurysms).zfill(2)\n",
    "        ni_img = nib.Nifti1Image(m, affine)\n",
    "        nib.save(ni_img, f\"../results/{cid}_{acc}_output.nii.gz\")\n",
    "        accepted_aneurysms += 1\n",
    "\n",
    "        #compute bounding box for aneurysm\n",
    "        bb = OBB(m, affine)\n",
    "        detected.append(bb.bounding_box())\n",
    "    \n",
    "    #if there is no detected aneurysm, store emtpy mask\n",
    "    if accepted_aneurysms == 0:\n",
    "        empty = np.zeros_like(labeled_array)\n",
    "        ni_img = nib.Nifti1Image(empty, affine)\n",
    "        nib.save(ni_img, f\"../results/{cid}_00_output.nii.gz\")\n",
    "        \n",
    "\n",
    "    \n",
    "    return {\n",
    "             \"dataset_id\":cid,\n",
    "             \"processing_time_in_seconds\":ptime,\n",
    "             \"candidates\":detected\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec06ccf",
   "metadata": {},
   "source": [
    "## 3. Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a084728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished run 1/22\n",
      "finished run 2/22\n",
      "finished run 3/22\n",
      "finished run 4/22\n",
      "finished run 5/22\n",
      "finished run 6/22\n",
      "finished run 7/22\n",
      "finished run 8/22\n",
      "finished run 9/22\n",
      "finished run 10/22\n",
      "finished run 11/22\n",
      "finished run 12/22\n",
      "finished run 13/22\n",
      "finished run 14/22\n",
      "finished run 15/22\n",
      "finished run 16/22\n",
      "finished run 17/22\n",
      "finished run 18/22\n",
      "finished run 19/22\n",
      "finished run 20/22\n",
      "finished run 21/22\n",
      "finished run 22/22\n"
     ]
    }
   ],
   "source": [
    "final = []\n",
    "for index, batch in enumerate(test_loader):\n",
    "    output = predict_on_batch(batch)\n",
    "    json_output = postprocess(output)\n",
    "    final.append(json_output)\n",
    "    print(f'finished run {index+1}/{len(test_loader)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f87360b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the final JSON output\n",
    "o = {\n",
    "       \"username\":\"acorn\",\n",
    "       \"task_1_results\": final\n",
    "    }\n",
    "\n",
    "with open('../results/detection.json', 'w') as f:\n",
    "    json.dump(o, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
